% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sql_upload_csv_chunked.R
\name{sql_upload_csv_chunked}
\alias{sql_upload_csv_chunked}
\title{Function to read csv file in chunks and upload to SQL & convert to parquet}
\usage{
sql_upload_csv_chunked(
  input_filename,
  col_list = NULL,
  con,
  schema = "dbo",
  table_name,
  truncate_table = FALSE,
  date_stamp = NULL,
  write_parquet = FALSE,
  backup_filepath = NULL,
  backup_name = NULL,
  pattern = NULL,
  file_remove = FALSE,
  chunk_size = 50000,
  skip = 0,
  tidy_names = FALSE
)
}
\arguments{
\item{input_filename}{file path to a .csv file}

\item{col_list}{list of columns a vroom or \link{readr} readr:::as.col_spec() object, default NULL
when \link{NULL} \link{readr} will auto detect column names
e.g. col_list <- cols_only(column = col_character()), see \code{\link[readr:cols]{readr::cols_only()}}
Please see \code{\link[readr:read_delim_chunked]{readr::read_delim_chunked()}} argument col_types to read up on this
To read in all cols as character use col_list = readr::cols(.default = col_character()) see \code{\link[readr:cols]{readr::cols()}}
readr will guess data types by the \code{\link[readr:read_delim_chunked]{readr::read_delim_chunked()}}'s guess_max parameter which is equal to the chunk size,
this is fast but unreliable particularly around integers, dates and float and may cause
inserts to fail}

\item{con}{a DBI con connection object \code{\link[DBI:dbConnect]{DBI::dbConnect()}}}

\item{schema}{the schema of the SQL table you are uploading to default "dbo"}

\item{table_name}{SQL destination table name, if does not exist will be created
by \code{\link[DBI:dbWriteTable]{DBI::dbWriteTable()}} in callback function}

\item{truncate_table}{boolean \link{TRUE}/ \link{FALSE}, \link{TRUE} will truncate the SQL table name supplied, default FALSE}

\item{date_stamp}{date stamp to stamp date file was loaded, suggest \code{\link[=Sys.Date]{Sys.Date()}}}

\item{write_parquet}{TRUE/ FALSE only, TRUE will write parquet files in chunks and combine them
without loading into local memory, default FALSE, FALSE will just preform SQL upload in callback}

\item{backup_filepath}{file path to folder or location where you want parquet backups to be written
do not include file name or .parquet extension as file names are created using pattern and name,
default NULL in case of write_parquet = FALSE}

\item{backup_name}{string name that the file name will be not including file path filepath is defined
by argument backup_file path
default \link{NULL} in case of write_parquet = FALSE}

\item{pattern}{a pattern to write temporary chunk parquet files with and to search for them when
combining the parquet files e.g. pattern = "temp" - looks for files with temp in the file name
or including file extension e.g. backup_name = "example_file" is for "blah/example_file.parquet"
default NULL in case of write_parquet = FALSE}

\item{file_remove}{removes parquet chunk files written in callback function, if argument
write_parquet = TRUE}

\item{chunk_size}{chunk size for \link{readr} to read .csv file and for callback to send to SQL,
default 50,000}

\item{skip}{Number of rows to skip, default 0}

\item{tidy_names}{tidy column names, removes and non-alpha_numeric character and replaces with "_",
e.g. example-colname will become example_colname\cr
This arguement is default FALSE}
}
\value{
TRUE invisibly and a SQL table filled with values from csv file and if write_parquet = TRUE
a parquet file backup of the .csv file
}
\description{
\ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#experimental}{\figure{lifecycle-experimental.svg}{options: alt='[Experimental]'}}}{\strong{[Experimental]}}
\itemize{
\item Function to read .csv files in chunks, upload to a destination SQL table, defined by
a DBI connection object. Can also write parquet backups of the .csv file which are a
more efficient way to store data than the .csv files. These are written in chunks by
the callback function used by \code{\link[readr:read_delim_chunked]{readr::read_csv_chunked()}} these can be combined and removed
if the file_remove argument is set to TRUE

\item This function uses \code{\link[readr:read_delim_chunked]{readr::read_csv_chunked()}} to read a csv file in chunks
without retaining the whole file in local memory to conserve RAM.
\code{\link[DBI:dbWriteTable]{DBI::dbWriteTable()}} with option append = TRUE is used to write each chunk
to a destination SQL table. For R based SQL inserts, it is recommended
that you use a staging table as R and SQL data types do not always align.
Parquet backup or file conversion functionality
This function has optional arguments to allow for the writing of chunked
parquet backups using \code{\link[arrow:write_parquet]{arrow::write_parquet()}} these are then combined using
\code{\link[arrow:open_dataset]{arrow::open_dataset()}} and \code{\link[arrow:write_parquet]{arrow::write_parquet()}} this is recommended because
parquet is a more efficient file format
than csv
\item argument date_stamp will create a column in the data.frame called date_stamp, if
you are using a pre-created SQL table ensure this column exists if argument is not NULL
}
}
\details{
Some internal parameters are fed to internal callback function. This function
should not need to be used by end users but this is a note for developers
see epidm:::callback()
}
\examples{
con <- DBI::dbConnect(RSQLite::SQLite(), ":memory:")
t <- tempdir(check = TRUE)
t1 <- paste0(t, "\\\\")
# TRUNCATE TABLE does not exist in SQLite so cannot be shown in examples
sql_upload_csv_chunked(readr::readr_example("mtcars.csv"),
  col_list = NULL,
  con = con,
  schema = NULL,
  table_name = "example",
  truncate_table = FALSE,
  write_parquet = TRUE,
  backup_filepath = t1,
  backup_name = "test_txt",
  pattern = "temp_chunk_parquet_file",
  file_remove = TRUE,
  chunk_size = 10
)

res <- DBI::dbReadTable(con, "example")
res <- dplyr::as_tibble(res) # Otherwise prints all 32 rows as df
res
arrow::read_parquet(paste0(t1, "test_txt.parquet"))
DBI::dbDisconnect(con)

}
\seealso{
\code{\link[=sql_upload_csv_chunked]{sql_upload_csv_chunked()}} \code{\link[=combine_parquet_files]{combine_parquet_files()}} \code{\link[=callback]{callback()}}
\code{\link[arrow:write_parquet]{arrow::write_parquet()}} \code{\link[arrow:read_parquet]{arrow::read_parquet()}} \code{\link[DBI:dbWriteTable]{DBI::dbWriteTable()}}
\code{\link[readr:read_delim_chunked]{readr::read_csv_chunked()}} \code{\link[=options]{options()}}
}
\author{
Owen Pullen
}
